---
title: "Practice"
author: "Sanaz Eghbali"
Due: "November 29, 2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
#Install and load required packages
```{r}
#install.packages("tm")
#install.packages("text2vec")
#install.packages("NLP")
library(tm)
library(text2vec)
library(NLP)
```


## Reading the Transcripts
```{r}
data <- read.csv(file = 'transcripts.csv', header = F, sep = '|')
doc <- 0
for (i in c(2:100)) {doc[i] <- as.character(data$V1[i])}
doc.list <- as.list(doc[2:100])
N.docs <- length(doc.list)
names(doc.list) <- paste0("Doc", c(1:N.docs))
Query <- as.character(data$V1[1])
```

## Preparing the Corpus
```{r}
my.docs <- VectorSource(c(doc.list, Query))
my.docs$Names <- c(names(doc.list), "Query")
my.corpus <- Corpus(my.docs)
my.corpus
```


## Cleaning and Preprocessing the text (Cleansing Techniques)
```{r}
#Write your answer here fro Question 1
#Hint: use getTransformations() function in tm Package
#https://cran.r-project.org/web/packages/tm/tm.pdf
#Remove Punctuation
getTransformations()
my.corpus<-tm_map(my.corpus, removePunctuation)
#Don't want to count separate words
#install.packages("SnowballC")
library(SnowballC)
my.corpus<-tm_map(my.corpus, stemDocument)
#Remove numbers and any extra white space 
my.corpus<-tm_map(my.corpus, removeNumbers)
my.corpus<-tm_map(my.corpus, content_transformer(tolower))
my.corpus<-tm_map(my.corpus, removeWords, stopwords(kind = "en"))
```

##Creating a uni-gram Term Document Matrix
```{r}
term.doc.matrix <- TermDocumentMatrix(my.corpus)
inspect(term.doc.matrix[1:10,1:10])
```

## Converting the generated TDM into a matrix and displaying the first 6 rows and the dimensions of the matrix
```{r}
term.doc.matrix <- as.matrix(term.doc.matrix)
head(term.doc.matrix)
dim(term.doc.matrix)
```

## Declaring weights (TF-IDF)
```{r}
get.tf.idf.weights <- function(tf.vec) {
  # Computes the tfidf weights from the term frequency vector
  n.docs <- length(tf.vec)
  doc.frequency <- length(tf.vec[tf.vec > 0])
  weights <- rep(0, length(tf.vec))
  relative.frequency <- tf.vec[tf.vec > 0] / sum(tf.vec[tf.vec > 0])
  weights[tf.vec > 0] <-  relative.frequency * log(n.docs/doc.frequency)
  return(weights)
}
```

## Declaring weights (TF-IDF variants)
```{r}
get.tf.idf.weights <- function(tf.vec) {
  # Computes the tfidf weights from the term frequency vector
  n.docs <- length(tf.vec)
  doc.frequency <- length(tf.vec[tf.vec > 0])
  weights <- rep(0, length(tf.vec))
  relative.frequency <- 1+log(tf.vec[tf.vec>0])
  weights[tf.vec > 0] <-  relative.frequency * log(1+(n.docs/doc.frequency))
  return(weights)
}

get.tf.idf.weights <- function(tf.vec) {
  # Computes the tfidf weights from the term frequency vector
  n.docs <- length(tf.vec)
  doc.frequency <- length(tf.vec[tf.vec > 0])
  weights <- rep(0, length(tf.vec))
  relative.frequency <- tf.vec[tf.vec > 0]
  weights[tf.vec > 0] <-  relative.frequency * log(n.docs/doc.frequency)
  return(weights)
}
```

###Computing Cosine Similarity and Displaying a heatmap
```{r}
tfidf.matrix <- t(apply(term.doc.matrix, 1,
                        FUN = function(row) {get.tf.idf.weights(row)}))

colnames(tfidf.matrix) <- my.docs$Names

head(tfidf.matrix)
dim(tfidf.matrix)


similarity.matrix <- sim2(t(tfidf.matrix), method = 'cosine')
heatmap(similarity.matrix)
```

##Showing the Results
```{r}
sort(similarity.matrix["Query", ], decreasing = TRUE)[1:10]
```

## Please use the following chunck to comment and conclude after conducting your comparative analyses
```{r}
# Doc99     Query
#1.0000000 1.0000000
#Doc99 and Query are the same
```